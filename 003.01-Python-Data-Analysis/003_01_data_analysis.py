
"""
003.01-Data-Analysis.ipynb

Automatically generated by Colaboratory.

"""

# Python Data Analysis - Module 3 of 5 in Py Machine Learning 
# Educative
# https://www.educative.io/module/Y6GKZ1igOAW0XJlmE/10370001/4599423935184896

"""# Basics

### Data Structures
"""

## List Refresher

# This creates the list
depths = [1, 5, 3, 6, 4, 7, 10, 12]

# This outputs the first 5 elements. No number before the : implies 0
first_5_depths = depths[:5]

print("---0---")
print(first_5_depths)

# You can easily sum
print("---1---")
print(sum(depths))

# And take the max
print("---2---")
print(max(depths))

# Slicing with a negative starts from the end, so this returns the last element
print("---3---")
print(depths[-1])

# This returns the end of the list starting from the second to the end
# Nothing after the : implies the end of the list
print("---4---")
print(depths[-2:])

# This returns the second, third, and forth elements
# Remember counting starts at zero!
print("---5---")
print(depths[2:5])

# These commands check if a value is contained in the list
print("---6---")
print(22 in depths)
print(1 in depths)

# This is how you add another value to the end of your list
depths.append(44)
print("---7---")
print(depths)

# You can extend a list with another list
depths.extend([100, 200])
print("---8---")
print(depths)

# You can also modify a value
# This replaces the 4th value with 100
depths[4] = 100
print("---9---")
print(depths)

# Or you can do insert to accomplish the same thing
depths.insert(5, 1000)
print("---10---")
print(depths)

## Dict refresher 

# Initialize the dictionary.
# Keys are first then a : then the value
my_dict = {"age": 22, "birth_year": 1999, "name": "jack", "siblings": ["jill", "jen"]}

# Get the value for the key age
print("---0---")
print(my_dict['age'])

# Check is age is a key
print("---1---")
print('age' in my_dict)

# Check is company is a key
print("---2---")
print('company' in my_dict)

# Get the value for they key age
print("---3---")
print(my_dict.get('age'))

# Get the value for they key company
# If it doesn't exsist, return 1
print("---4---")
print(my_dict.get('company', 1))

# Return all the keys
print("---5---")
print(my_dict.keys())

# Return all the values
print("---6---")
print(my_dict.values())

# Return all the key, value pairs
print("---7---")
print(my_dict.items())

"""
## Default Dict to auto populate defaults

Example: specifies type :int: and value 0 for default
"""
from collections import defaultdict  # import defaultdict class

my_default_dict = defaultdict(int)   # make a default dictionary
my_default_dict['age'] = 22          # adding a key-value pair

print(my_default_dict['company'])    # printing the value of the key "company"

"""
## Sets

Unique values only.
Can perform union, intersection, difference methods
"""

my_set = set()
my_set.add(1)
my_set.add(2)
my_set.add(1)
# Note that the set only contains a single 1 value
print("---0---")
print(my_set)

my_set2 = set()
my_set2.add(1)
my_set2.add(2)
my_set2.add(3)
my_set2.add(4)
print("---1---")
print(my_set2)

# Prints the overlap
print("---1---")
print(my_set.intersection(my_set2))
print("---2---")

# Prints the combination
print(my_set.union(my_set2))

# Prints the difference (those in my_set but not my_set2)
print("---3---")
print(my_set.difference(my_set2))

"""### Numpy crash course"""

# https://numpy.org/doc/stable/user/index.html

import numpy as np

# This creates our array
np_array = np.array([5, 10, 15, 20, 25, 30])
print("--0--")

# Gets the unique values
print(np.unique(np_array))
print("--1--")

# Calculates the standard deviation
print(np.std(np_array))
print("--2--")

# Calculates the maximum
print(np_array.max())
print("--3--")

# Squares each value in the array
print(np_array ** 2)
print("--4--")

# Adds the arrays together element wise
print(np_array + np_array)
print("--5--")

# The sum of the squares of the elements
print(np.sum(np_array ** 2))
print("--6--")

# Gives you the shape: (rows, columns)
print(np_array.shape)

import numpy as np

# Create 2d array
print("--0--")
np_2d_array = np.array([[1,2,3], 
                        [4,5,6]])
print(np_2d_array)

# Calculate the transpose, which is when you swap the columns and rows.
print("--1--")
np_2d_array_T = np_2d_array.T
print(np_2d_array_T)

# Print the shape of the array as (number of rows, number of columns)
print("--3--")
print(np_2d_array.shape)

# Access elements in the 2d array by index. 
# First index is the row number
# Second index is the column number
# Index numbers start from 0
print("--4--")
print(np_2d_array[1,1])
print(np_2d_array[0,2])

"""
## Dot product
# https://en.wikipedia.org/wiki/Dot_product

In mathematics, the dot product or scalar product[note 1] is an algebraic operation that takes two equal-length sequences of numbers 
(usually coordinate vectors), and returns a single number.
"""

import numpy as np

np_array = np.array([5, 10, 15, 20, 25, 30])
dot_product = np.dot(np_array, np_array)

print(dot_product)

## Randoms in Numpy

import numpy as np

# Generage a single random number in range [0,1)
print("--0--")
print(np.random.rand())

# Generate a matrix of random numbers in range [0,1) with shape (3,2)
print("--1--")
print(np.random.rand(3,2))

import numpy as np

# Low=5, High=15, Size=2. Generate 2 values between 5 and 15 (exclusive)
print("--0--")
print(np.random.randint(5, 15, 2))

# Low=5, High=15, Size=(3,2). Generate a matrix of shape (3,2) with values between 5 and 15 (exclusive)
print("--1--")
print(np.random.randint(5, 15, (3,2)))

"""
## Choice

The choice() function allows you to pass an array, 
specify how many values to sample, 
and decide whether sampling should be done with or without replacement. 
Sampling without replacement means the same value can’t be sampled more than once.
"""

import numpy as np

array = np.array([1,2,3,4,5])

# Sample 10 data points with replacement. 
print("--0--")
print(np.random.choice(array, 10, replace=True))

# Sample 3 data points without replacement. 
print("--1--")
print(np.random.choice(array, 3, replace=False))

"""### SciPy"""

"""
# Correlation with Pearson r
# https://en.wikipedia.org/wiki/Pearson_correlation_coefficient

FYI: p-value

The p-value for the permutation test is the proportion of the r values generated in step (2) 
that are larger than the Pearson correlation coefficient that was calculated from the original data. 
Here "larger" can mean either that the value is larger in magnitude, 
or larger in signed value, depending on whether a two-sided or one-sided test is desired.


"""



from scipy import stats
import numpy as np

array_1 = np.array([1,2,3,4,5,6])  # Create a numpy array from a list
array_2 = array_1  # Create another array with the same values

print(stats.pearsonr(array_1, array_2))  # Calculate the correlation which will be 1 since the values are the same

"""
# Standard Deviation - Normal Distribution - Gaussian Distribution
# https://en.wikipedia.org/wiki/Normal_distribution


The code above uses the loc parameter as the mean, 
the scale as the standard deviation, 
and the size as the number of samples to return. 

If you sampled enough data points and plotted the results, 
you would see a normal distribution centered around 0 with a standard deviation of 10.

"""

from scipy import stats

x = stats.norm.rvs(loc=0, scale=10, size=10)  # Generate 10 values randomly sampled from a normal distribution with mean 0 and standard deviation of 10

print(x)

"""
# Prob Density Function 

p1 >> less likely
p2 >> very likely

"""
from scipy import stats

p1 = stats.norm.pdf(x=-100, loc=0, scale=10)  # Get probability of sampling a value of -100
p2 = stats.norm.pdf(x=0, loc=0, scale=10)     # Get probability of sampling a value of 0

print(p1)
print(p2)

"""
# Cumulative Distribution Function 

We can see that the cumulative distribution function with x=0 is 0.5 
because 0 is the mean and with a normal distribution, 
half of the data is less than or equal to the mean.


"""
from scipy import stats

p1 = stats.norm.cdf(x=0, loc=0, scale=10)  # Get probability of sampling a value less than or equal to 0

print(p1)

"""
# Describe stats

The variance is the square of the standard deviation. 

The skewness is a measure of the asymmetry of the distribution. 

The kurtosis is a measure of the “tailedness” of the distribution. 

A large value usually means there are more outliers.


"""
from scipy import stats

print(stats.describe(stats.norm.rvs(loc=0, scale=1, size=500)))  # Calculate descriptive statistics for 500 data points sampled from normal distribution with mean 0 and standard deviation of 1

"""# Reading Data

### .csv files
"""

"""
# Read a .csv with Pandas

Verify adult.data file is in session runtime directory

"""
import pandas as pd

# Define the column names as a list
names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',
        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']
# Read in the CSV file from the webpage using the defined column names
df = pd.read_csv("adult.data", header=None, names=names) # Uncomment if file is valid and local
                      
print(df.head()) # uncomment

help(pd.read_csv)

"""### .json"""

"""
# String json
"""

import json

## Define the JSON object as a string
json_string = """{
    "glossary": {
        "title": "example glossary",
        "GlossDiv": {
            "title": "S",
            "GlossList": {
                "GlossEntry": {
                    "ID": "SGML",
                    "SortAs": "SGML",
                    "GlossTerm": "Standard Generalized Markup Language",
                    "Acronym": "SGML",
                    "Abbrev": "ISO 8879:1986",
                    "GlossDef": {
                        "para": "A meta-markup language, used to create markup languages such as DocBook.",
                        "GlossSeeAlso": ["GML", "XML"]
                    },
                    "GlossSee": "markup"
                }
            }
        }
    }
}"""


# Read the JSON data into Python
json_data = json.loads(json_string)

print(json_data)

# load a json string or bytes or bytearray
help(json.loads)

"""
# Load a .json file
"""

# with open('data.json') as f:
#     data = json.load(f)

help(json.load)

"""### raw data"""

import tempfile

tmp = tempfile.NamedTemporaryFile()

# Open (create) the file for writing. And write the data.
with open(tmp.name, 'w') as f:
    f.write("James|22|M\n")
    f.write("Sarah|31|F\n")
    f.write("Mindy|25|F")

# Read in the data from our file, line by line
with open(tmp.name, "r") as f:
    for line in f:
      print(line)

help(tempfile)

"""### Exercise"""

import pandas as pd

def read_csv(fp):
    cols = ["mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model_year", "origin", "car_name"]
    df = pd.read_csv(fp, delim_whitespace=True, header=None, names = cols)

    return df

"""
# shape
"""
fp="auto-mpg.data"
read_csv(fp)

"""
# head
"""
read_csv(fp).head()

"""# Describing Data"""

import pandas as pd

fp = "auto-mpg.data"
cols = ["mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model_year", "origin", "car_name"]
df = pd.read_csv(fp, delim_whitespace=True, header=None, names = cols)

df.describe()

"""
# Unique data types
"""

import pandas as pd
names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',
        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']
train_df = pd.read_csv("adult.data", header=None, names=names)
print(train_df['relationship'].unique())

"""
# Value counts
"""

import pandas as pd

names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',
        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']
train_df = pd.read_csv("adult.data", header=None, names=names)

print(train_df['relationship'].value_counts())

"""
# Group by 
"""

import pandas as pd
names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',
        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']
train_df = pd.read_csv("adult.data", header=None, names=names)

# Group by relationship and then get the value counts of label with normalization                   
print(train_df.groupby('relationship')['label'].value_counts(normalize=True))

"""
# Correlation
"""

import pandas as pd

names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',
        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']
train_df = pd.read_csv("adult.data", header=None, names=names)

# Calculate correlations                   
print(train_df.corr())

"""
# Binarization of column label 
"""

import pandas as pd

names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',
        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']
train_df = pd.read_csv("adult.data", header=None, names=names)

# Convert the string label into a value of 1 when >= 50k and 0 otherwise
train_df['label_int'] = train_df.label.apply(lambda x: ">" in x)

print(train_df.corr())

"""
# Percentiles
"""

import pandas as pd

names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',
        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']
train_df = pd.read_csv("adult.data", header=None, names=names)

# Use the describe function to calculate the percentiles specified                     
print(train_df.describe(percentiles=[.01,.05,.95,.99]))

"""
# Pivot Table
"""

import numpy as np
import pandas as pd

names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',
        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']
train_df = pd.read_csv("adult.data", header=None, names=names)

# Pivot the data frame to show by relationship, workclass (rows) and label (columns) the average hours per week.
print(pd.pivot_table(train_df, values='hoursperweek', index=['relationship','workclass'], 
               columns=['label'], aggfunc=np.mean).round(2))

help(pd.pivot_table)

"""
# Cross tab
"""
import numpy as np
import pandas as pd

names = ['age', 'workclass', 'fnlwgt', 'education', 'educationnum', 'maritalstatus', 'occupation', 'relationship', 'race',
        'sex', 'capitalgain', 'capitalloss', 'hoursperweek', 'nativecountry', 'label']
train_df = pd.read_csv("adult.data", header=None, names=names)

# Calculate the frequencies between label and relationship
print(pd.crosstab(train_df['label'], train_df['relationship']))

"""
# Crosstab with normalized outputs
"""
print(pd.crosstab(train_df['label'], train_df.relationship, normalize=True))

"""# Cleaning Data"""

"""
## Fill na
"""

import numpy as np
import pandas as pd

pd_series = pd.Series([5, 10, np.nan, 15, 20, np.nan, 25, 50, np.nan])
print("Average of non-missing values: {0}".format(pd_series.mean()))

pd_series = pd_series.fillna(pd_series.mean())
print(pd_series)

"""
## Drop na
"""

import numpy as np
import pandas as pd

# Create a series with missing data
pd_series = pd.Series([5, 10, np.nan, 15, 20, np.nan, 25, 50, np.nan])
# Drop rows with missing data
pd_series = pd_series.dropna()
print(pd_series)

"""
## isnull()
"""

import numpy as np
import pandas as pd

# Create a series with missing values
pd_series = pd.Series([5, 10, np.nan, 15, 20, np.nan, 25, 50, np.nan])
# Show which rows are missing
print(pd_series.isnull())

"""## Scaling"""

"""
# Standard Scaling
# Standard scaling subtracts the mean and divides by the standard deviation. 
# This centers the feature on zero with unit variance.
"""

from sklearn.preprocessing import MinMaxScaler, StandardScaler
import numpy as np

# Create a matrix of data
data = [[-1, 2], 
        [-0.5, 6], 
        [0, 10], 
        [1, 18]]

print("Before Standard scaling")
print(np.mean(data, 0))
print(np.std(data, 0))

# Initalize a StandardScaler
standard = StandardScaler()
# Fit and transform the data with the StandardScaler
standard_data = standard.fit_transform(data)

print("After Standard scaling")
print(np.mean(standard_data, 0))
print(np.std(standard_data, 0))

"""In the example above, we created a NumPy array of shape (4,2). 

We then use the StandardScaler() from sklearn which will automatically subtract the mean and divide by the standard deviation of each of our columns. 

This is done with the fit_transform() call.
"""

"""
# Min max scaling
"""

from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Create matrix of data
data = [[-1, 2], 
        [-0.5, 6], 
        [0, 10], 
        [1, 18]]

# Initalize MinMaxScaler
min_max = MinMaxScaler()
# Fit and transform the data
min_max_data = min_max.fit_transform(data)

print(np.min(min_max_data, 0))
print(np.max(min_max_data, 0))
print(np.mean(min_max_data, 0))
print(np.std(min_max_data, 0))

"""## Categorial"""

"""
# Label encoding
"""
import pandas as pd

# Create series with male and female values
non_categorical_series = pd.Series(['male', 'female', 'male', 'female'])
# Convert the text series to a categorical series
categorical_series = non_categorical_series.astype('category')
# Print the numeric codes for each value
print(categorical_series.cat.codes)
# Print the category names
print(categorical_series.cat.categories)

"""
## One hot encoding -- similar, save creates new col for 1s and 0s
"""

import pandas as pd

# Create series with male and female values
non_categorical_series = pd.Series(['male', 'female', 'male', 'female'])
# Create dummy or one-hot encoded variables
print(pd.get_dummies(non_categorical_series))

# get_dummies()

"""
# Outlier detection
# 90th and 10th percentile
"""

def outlier_detection(df):
    df = df.quantile([.90, .10])
    return df

help(pd.DataFrame.quantile)

"""# Visualization

`Matplotlib` is sort of the base plotting library in Python. Think of it as a low-level library that allows you to do all sorts of things, but this flexibility can sometimes make it hard to work with. Matplotlib has been around for a while and sometimes can look a bit dated in style.

`Seaborn` was created to help deal with some of these issues. It is built on top of Matplotlib and in its own words, “provides a high-level interface for drawing attractive statistical graphics.” For the most part, when possible, I lean towards using Seaborn. When I need more low-level control, I pull in Matplotlib. Since Seaborn is built on top of Matplotlib, it is pretty easy to mix the two.
"""

"""
# Data set for demo

  Scatter
  Bar
  Distribution
  Line
  Heat map
  Data aware

"""
import pandas as pd

# Load the boston dataset from sklearn.datasets
from sklearn.datasets import fetch_california_housing

housing = fetch_california_housing()
# Enter the boston data into a dataframe
housing_df = pd.DataFrame(housing.data, columns=housing.feature_names)

# Print the first 5 rows to confirm ran correctly
print(housing_df.head())

"""## Scatter"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Set the palette and style to be more minimal
sns.set(style='ticks', palette='Set2')

# Create the scatter plot
sns.lmplot(x="HouseAge", y="AveRooms", data=housing_df)
# Remove excess chart lines and ticks for a nicer looking plot
sns.despine()

"""## Distributions

# Probability distribution
# https://en.wikipedia.org/wiki/Probability_distribution

A probability distribution is a mathematical function that provides the probabilities of the occurrence of different possible outcomes.

Plotting Distributions:

    Histograms
    Box plots
    Violin plots
    Joint plots

""
"""

"""
## Histogram

As explained by Wikipedia, a histogram is an estimate of the probability distribution of a continuous variable. To construct a histogram, the first step is to bin the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable.

You create a histogram with the distplot() function is Seaborn. You only need to pass one argument which is the continuous variable for which you would like to construct a histogram.

"""

# from sklearn.datasets import load_boston
# import pandas as pd
# import seaborn as sns
# import matplotlib.pyplot as plt
# # Set the palette and style to be more minimal
# sns.set(style='ticks', palette='Set2')

# # Load data as explained in introductory lesson
# boston_data = load_boston()
# boston_df = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)

# # Create the histogram plot
# sns.distplot(boston_df.NOX, kde=False)
# # Remove excess chart lines and ticks for a nicer looking plot
# sns.despine()

"""
## Boxplot

Seaborn also has a box plot. A box plot is a graphical view of summary statistics from a distribution.

"""

# from sklearn.datasets import load_boston
# import pandas as pd
# import seaborn as sns
# import matplotlib.pyplot as plt
# # Set the palette and style to be more minimal
# sns.set(style='ticks', palette='Set2')

# # Load data as explained in introductory lesson
# boston_data = load_boston()
# boston_df = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)

# # Create the box plot
# sns.boxplot(boston_df.NOX)
# # Remove excess chart lines and ticks for a nicer looking plot
# sns.despine()

"""
## Line Graphs
"""

import seaborn as sns            # importing seaborn functionality    
import pandas as pd
import matplotlib.pyplot as plt
 
flights_long=sns.load_dataset("flights")   # importing dataset
 
# filtering the dataset to obtain the January records for all years
flights_long=flights_long[flights_long.month == 'January']

#plotting a line graph
plot=sns.lineplot(x=flights_long.year, y=flights_long.passengers, )

help(sns.lineplot)

"""
## Heatmaps
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
flights_long = sns.load_dataset("flights")
# Pivot the dataset from long to wide format
flights = flights_long.pivot("month", "year", "passengers")
# Create a larger figure size to plot on
f, ax = plt.subplots(figsize=(12, 6))
# Create the heat map
sns.heatmap(flights, annot=True, fmt="d", linewidths=.5, ax=ax, cmap='Blues')